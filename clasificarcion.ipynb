{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from models import LIMUBertModel4Pretrain, ClassifierGRU\n",
    "from config import PretrainModelConfig, ClassifierModelConfig, load_model_config, load_dataset_stats\n",
    "\n",
    "# Asumimos que estamos en un notebook de Colab/Jupyter\n",
    "\n",
    "# 1. Configuración y carga de modelos\n",
    "def load_models(bert_path, classifier_path, dataset, version):\n",
    "    # Cargar configuraciones\n",
    "    bert_cfg = load_model_config('pretrain', 'base', 'v1', path_bert='config/limu_bert.json')\n",
    "    classifier_cfg = load_model_config('classifier', 'gru', 'v1', path_classifier='config/classifier.json')\n",
    "    \n",
    "    # Imprimir la configuración del clasificador para verificar\n",
    "    print(\"Classifier config:\", classifier_cfg)\n",
    "    \n",
    "    # Inicializar modelos\n",
    "    bert_model = LIMUBertModel4Pretrain(bert_cfg, output_embed=True)\n",
    "    # Asumiendo que el modelo guardado tiene una capa lineal final de 6 a 10\n",
    "    output_dim = 6  # o el valor correcto basado en tu modelo guardado\n",
    "    classifier_model = ClassifierGRU(classifier_cfg, input=bert_cfg.hidden, output=output_dim)\n",
    "    #classifier_model = ClassifierGRU(classifier_cfg, input=bert_cfg.hidden, output=classifier_cfg.linear_io[-1][1])\n",
    "    \n",
    "    # Imprimir la estructura del modelo para verificar\n",
    "    print(classifier_model)\n",
    "    \n",
    "    # Cargar pesos pre-entrenados\n",
    "    bert_model.load_state_dict(torch.load(bert_path, map_location='cpu'))\n",
    "    \n",
    "    # Cargar el estado del clasificador\n",
    "    classifier_state = torch.load(classifier_path, map_location='cpu')\n",
    "    \n",
    "    # Imprimir las claves y formas del estado del clasificador\n",
    "    for key, value in classifier_state.items():\n",
    "        print(f\"{key}: {value.shape}\")\n",
    "    \n",
    "    # Intenta cargar el estado del clasificador\n",
    "    classifier_model.load_state_dict(classifier_state, strict=False)\n",
    "    \n",
    "    return bert_model, classifier_model\n",
    "\n",
    "# 2. Preprocesamiento de datos\n",
    "def preprocess_data(raw_data, window_size=120, stride=60):\n",
    "    windows = []\n",
    "    for i in range(0, len(raw_data) - window_size + 1, stride):\n",
    "        windows.append(raw_data[i:i+window_size])\n",
    "    return np.array(windows)\n",
    "\n",
    "# 3. Inferencia\n",
    "def inference(bert_model, classifier_model, input_data):\n",
    "    bert_model.eval()\n",
    "    classifier_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embeddings = bert_model(torch.FloatTensor(input_data))\n",
    "        outputs = classifier_model(embeddings)\n",
    "        \n",
    "    return torch.softmax(outputs, dim=1).numpy()\n",
    "\n",
    "# 4. Función principal modificada\n",
    "def classify_imu_data(raw_data, seconds, sample_rate, bert_path, classifier_path, dataset, version):\n",
    "    # Cargar modelos\n",
    "    bert_model, classifier_model = load_models(bert_path, classifier_path, dataset, version)\n",
    "    \n",
    "    # Cargar configuración del dataset\n",
    "    dataset_config = load_dataset_stats(dataset, version)\n",
    "    \n",
    "    # Preprocesar datos\n",
    "    n_samples = seconds * sample_rate\n",
    "    data = raw_data[:n_samples]  # Tomar solo los primeros N segundos\n",
    "    preprocessed_data = preprocess_data(data, window_size=dataset_config.seq_len, stride=dataset_config.seq_len//2)\n",
    "    \n",
    "    # Realizar inferencia\n",
    "    predictions = inference(bert_model, classifier_model, preprocessed_data)\n",
    "    \n",
    "    # Agregar predicciones\n",
    "    final_prediction = np.mean(predictions, axis=0)\n",
    "    \n",
    "    # Obtener la categoría con la probabilidad más alta\n",
    "    category_index = np.argmax(final_prediction)\n",
    "    category = dataset_config.activity_label[category_index]\n",
    "    \n",
    "    return category, final_prediction\n",
    "\n",
    "# 5. Función para imprimir resultados detallados\n",
    "def print_detailed_results(category, probabilities, dataset_config):\n",
    "    print(f\"Categoría predicha: {category}\")\n",
    "    print(\"\\nProbabilidades por categoría:\")\n",
    "    for label, prob in zip(dataset_config.activity_label, probabilities):\n",
    "        print(f\"{label}: {prob:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de uso modificado\n",
    "BERT_PATH = 'saved/pretrain_base_motion_20_120/motion.pt'\n",
    "CLASSIFIER_PATH = 'saved/classifier_base_gru_motion_20_120/motion.pt'\n",
    "DATASET = 'hhar'\n",
    "VERSION = '20_120'\n",
    "\n",
    "# Simular datos de IMU (reemplaza esto con tus datos reales)\n",
    "raw_imu_data = np.random.randn(120, 6)  # 6 seg de datos a 20Hz\n",
    "\n",
    "# Clasificar 10 segundos de datos\n",
    "category, probabilities = classify_imu_data(raw_imu_data, seconds=10, sample_rate=100, \n",
    "                                            bert_path=BERT_PATH, classifier_path=CLASSIFIER_PATH,\n",
    "                                            dataset=DATASET, version=VERSION)\n",
    "\n",
    "# Cargar la configuración del dataset para obtener las etiquetas\n",
    "dataset_config = load_dataset_stats(DATASET, VERSION)\n",
    "\n",
    "# Imprimir resultados detallados\n",
    "print_detailed_results(category, probabilities, dataset_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## desacoplando\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from models import LIMUBertModel4Pretrain, ClassifierGRU\n",
    "from config import PretrainModelConfig, ClassifierModelConfig, load_model_config, load_dataset_stats\n",
    "\n",
    "\n",
    "# 1. Configuración y carga de modelos\n",
    "def load_models(bert_path, classifier_path, dataset, version):\n",
    "    # Cargar configuraciones\n",
    "    bert_cfg = load_model_config('pretrain', 'base', 'v1', path_bert='config/limu_bert.json')\n",
    "    classifier_cfg = load_model_config('classifier', 'gru', 'v1', path_classifier='config/classifier.json')\n",
    "    \n",
    "    # Imprimir la configuración del clasificador para verificar\n",
    "    print(\"Classifier config:\", classifier_cfg)\n",
    "    \n",
    "    # Inicializar modelos\n",
    "    bert_model = LIMUBertModel4Pretrain(bert_cfg, output_embed=True)\n",
    "    # Asumiendo que el modelo guardado tiene una capa lineal final de 6 a 10\n",
    "    output_dim = 6  # o el valor correcto basado en tu modelo guardado\n",
    "    classifier_model = ClassifierGRU(classifier_cfg, input=bert_cfg.hidden, output=output_dim)\n",
    "    #classifier_model = ClassifierGRU(classifier_cfg, input=bert_cfg.hidden, output=classifier_cfg.linear_io[-1][1])\n",
    "    \n",
    "    # Imprimir la estructura del modelo para verificar\n",
    "    print(classifier_model)\n",
    "    \n",
    "    # Cargar pesos pre-entrenados\n",
    "    bert_model.load_state_dict(torch.load(bert_path, map_location='cpu'))\n",
    "    \n",
    "    # Cargar el estado del clasificador\n",
    "    classifier_state = torch.load(classifier_path, map_location='cpu')\n",
    "    \n",
    "    # Imprimir las claves y formas del estado del clasificador\n",
    "    for key, value in classifier_state.items():\n",
    "        print(f\"{key}: {value.shape}\")\n",
    "    \n",
    "    # Intenta cargar el estado del clasificador\n",
    "    classifier_model.load_state_dict(classifier_state, strict=False)\n",
    "    \n",
    "    return bert_model, classifier_model\n",
    "\n",
    "# 2. Preprocesamiento de datos\n",
    "def preprocess_data(raw_data, window_size=120, stride=60):\n",
    "    windows = []\n",
    "    for i in range(0, len(raw_data) - window_size + 1, stride):\n",
    "        windows.append(raw_data[i:i+window_size])\n",
    "    return np.array(windows)\n",
    "\n",
    "# 3. Inferencia (modificada para aceptar modelos precargados)\n",
    "def inference(bert_model, classifier_model, input_data):\n",
    "    bert_model.eval()\n",
    "    classifier_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embeddings = bert_model(torch.FloatTensor(input_data))\n",
    "        outputs = classifier_model(embeddings)\n",
    "        \n",
    "    return torch.softmax(outputs, dim=1).numpy()\n",
    "\n",
    "# 4. Función de clasificación (modificada para aceptar modelos precargados)\n",
    "def classify_imu_data(raw_data, seconds, sample_rate, bert_model, classifier_model, dataset_config):\n",
    "    # Preprocesar datos\n",
    "    n_samples = seconds * sample_rate\n",
    "    data = raw_data[:n_samples]  # Tomar solo los primeros N segundos\n",
    "    preprocessed_data = preprocess_data(data, window_size=dataset_config.seq_len, stride=dataset_config.seq_len//2)\n",
    "    \n",
    "    # Realizar inferencia\n",
    "    predictions = inference(bert_model, classifier_model, preprocessed_data)\n",
    "    \n",
    "    # Agregar predicciones\n",
    "    final_prediction = np.mean(predictions, axis=0)\n",
    "    \n",
    "    # Obtener la categoría con la probabilidad más alta\n",
    "    category_index = np.argmax(final_prediction)\n",
    "    category = dataset_config.activity_label[category_index]\n",
    "    \n",
    "    return category, final_prediction\n",
    "\n",
    "\n",
    "# 5. Función para imprimir resultados detallados\n",
    "def print_detailed_results(category, probabilities, dataset_config):\n",
    "    print(f\"Categoría predicha: {category}\")\n",
    "    print(\"\\nProbabilidades por categoría:\")\n",
    "    for label, prob in zip(dataset_config.activity_label, probabilities):\n",
    "        print(f\"{label}: {prob:.4f}\")\n",
    "\n",
    "\n",
    "# 6. Nueva función para cargar modelos y configuración\n",
    "def load_models_and_config(bert_path, classifier_path, dataset, version):\n",
    "    bert_model, classifier_model = load_models(bert_path, classifier_path, dataset, version)\n",
    "    dataset_config = load_dataset_stats(dataset, version)\n",
    "    return bert_model, classifier_model, dataset_config\n",
    "\n",
    "# Función para realizar múltiples inferencias\n",
    "def run_multiple_inferences(num_inferences):\n",
    "    for i in range(num_inferences):\n",
    "        # Simular datos de IMU (reemplaza esto con tus datos reales)\n",
    "        raw_imu_data = np.random.randn(6000, 6)  # 1 minuto de datos a 100Hz\n",
    "\n",
    "        # Clasificar 10 segundos de datos\n",
    "        category, probabilities = classify_imu_data(raw_imu_data, seconds=10, sample_rate=100, \n",
    "                                                    bert_model=bert_model, classifier_model=classifier_model,\n",
    "                                                    dataset_config=dataset_config)\n",
    "\n",
    "        print(f\"\\nInferencia {i+1}:\")\n",
    "        print_detailed_results(category, probabilities, dataset_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ejemplo de uso modificado\n",
    "BERT_PATH = 'saved/pretrain_base_motion_20_120/motion.pt'\n",
    "CLASSIFIER_PATH = 'saved/classifier_base_gru_motion_20_120/motion.pt'\n",
    "DATASET = 'hhar'\n",
    "VERSION = '20_120'\n",
    "\n",
    "# Cargar modelos y configuración una sola vez\n",
    "bert_model, classifier_model, dataset_config = load_models_and_config(BERT_PATH, CLASSIFIER_PATH, DATASET, VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_imu_data = np.random.randn(120, 6)  # 6 seg de datos a 20Hz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categoría predicha: bike\n",
      "\n",
      "Probabilidades por categoría:\n",
      "bike: 0.5448\n",
      "sit: 0.1550\n",
      "downstairs: 0.0097\n",
      "upstairs: 0.0650\n",
      "stand: 0.0950\n",
      "walk: 0.1306\n"
     ]
    }
   ],
   "source": [
    "category, probabilities = classify_imu_data(raw_imu_data, seconds=6, sample_rate=20,bert_model=bert_model, classifier_model=classifier_model,dataset_config=dataset_config)\n",
    "print_detailed_results(category, probabilities, dataset_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inferencia 1:\n",
      "Categoría predicha: bike\n",
      "\n",
      "Probabilidades por categoría:\n",
      "bike: 0.4348\n",
      "sit: 0.1414\n",
      "downstairs: 0.0201\n",
      "upstairs: 0.0569\n",
      "stand: 0.0901\n",
      "walk: 0.2567\n",
      "\n",
      "Inferencia 2:\n",
      "Categoría predicha: bike\n",
      "\n",
      "Probabilidades por categoría:\n",
      "bike: 0.4334\n",
      "sit: 0.1562\n",
      "downstairs: 0.0246\n",
      "upstairs: 0.0897\n",
      "stand: 0.0987\n",
      "walk: 0.1973\n",
      "\n",
      "Inferencia 3:\n",
      "Categoría predicha: bike\n",
      "\n",
      "Probabilidades por categoría:\n",
      "bike: 0.5128\n",
      "sit: 0.1500\n",
      "downstairs: 0.0133\n",
      "upstairs: 0.0634\n",
      "stand: 0.1001\n",
      "walk: 0.1604\n"
     ]
    }
   ],
   "source": [
    "run_multiple_inferences(3)  # Cambia el número según cuántas inferencias quieras realizar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "# En duda de uso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Funciones proporcionadas\n",
    "def load_config(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        config = json.load(f)\n",
    "    return config\n",
    "\n",
    "def load_data_and_labels(dataset_name):\n",
    "    cwd = os.getcwd()\n",
    "    config_path = os.path.join(cwd, 'dataset/data_config.json')\n",
    "    config = load_config(config_path)\n",
    "\n",
    "    data_info = config[dataset_name]\n",
    "    sr = data_info['sr']\n",
    "    seq_len = data_info['seq_len']\n",
    "    dimension = data_info['dimension']\n",
    "    activity_label_index = data_info['activity_label_index']\n",
    "    activity_label = data_info['activity_label']\n",
    "    user_label_index = data_info.get('user_label_index')\n",
    "    user_label = data_info.get('user_label')\n",
    "    \n",
    "    data_file = f'data_{sr}_{seq_len}.npy'\n",
    "    label_file = f'label_{sr}_{seq_len}.npy'\n",
    "    \n",
    "    dataset_folder = os.path.join(cwd,\"dataset/\",dataset_name.split('_')[0])\n",
    "    data_path = os.path.join(cwd, dataset_folder, data_file)\n",
    "    label_path = os.path.join(cwd, dataset_folder, label_file)\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"Data file not found: {data_path}\")\n",
    "    if not os.path.exists(label_path):\n",
    "        raise FileNotFoundError(f\"Label file not found: {label_path}\")\n",
    "\n",
    "    data = np.load(data_path)\n",
    "    labels = np.load(label_path)\n",
    "    \n",
    "    return data, labels, activity_label_index, activity_label, user_label_index, user_label\n",
    "\n",
    "def create_dataframe(data, labels, activity_label_index, activity_label):\n",
    "    n_samples, n_windows, n_features = data.shape\n",
    "    time_index = np.arange(n_samples * n_windows) / (n_windows // 10)  # Calcula los tiempos en segundos\n",
    "    \n",
    "    # Aplanar los datos y las etiquetas\n",
    "    flattened_data = data.reshape(n_samples * n_windows, n_features)\n",
    "    flattened_labels = labels.reshape(n_samples * n_windows, -1)\n",
    "    \n",
    "    # Crear el DataFrame con los datos y las etiquetas\n",
    "    df = pd.DataFrame(flattened_data, columns=[f'DOF{i+1}' for i in range(n_features)])\n",
    "    df['Time (s)'] = time_index\n",
    "    df['Activity'] = [activity_label[int(label)] for label in flattened_labels[:, activity_label_index]]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def plot_data(df, start_second=None, end_second=None, figsize=(12, 8), show_milliseconds=False):\n",
    "    if start_second is None:\n",
    "        start_second = df['Time (s)'].min()\n",
    "    if end_second is None:\n",
    "        end_second = df['Time (s)'].max()\n",
    "\n",
    "    df_filtered = df[(df['Time (s)'] >= start_second) & (df['Time (s)'] <= end_second)]\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    for i in range(1, df.shape[1] - 1):  # Excluye la columna 'Activity'\n",
    "        plt.subplot(3, 2, i)\n",
    "        for activity in df['Activity'].unique():\n",
    "            activity_data = df_filtered[df_filtered['Activity'] == activity]\n",
    "            plt.plot(activity_data['Time (s)'], activity_data[f'DOF{i}'], label=activity)\n",
    "        plt.title(f'DOF{i}')\n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.ylabel(f'DOF{i}')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.xticks(ticks=plt.xticks()[0], labels=[f'{x:.1f}s' for x in plt.xticks()[0]])\n",
    "        if show_milliseconds:\n",
    "            plt.xticks(ticks=plt.xticks()[0], labels=[f'{x:.1f}s ({x*1000:.0f}ms)' for x in plt.xticks()[0]])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_activity(df, activity, start_second=None, end_second=None, figsize=(12, 8)):\n",
    "    if start_second is None:\n",
    "        start_second = df['Time (s)'].min()\n",
    "    if end_second is None:\n",
    "        end_second = df['Time (s)'].max()\n",
    "\n",
    "    df_filtered = df[(df['Time (s)'] >= start_second) & (df['Time (s)'] <= end_second) & (df['Activity'] == activity)]\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    # Subgráfico para el acelerómetro (3 DOF)\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(df_filtered['Time (s)'], df_filtered['DOF1'], label=f'{activity} - DOF1')\n",
    "    plt.plot(df_filtered['Time (s)'], df_filtered['DOF2'], label=f'{activity} - DOF2')\n",
    "    plt.plot(df_filtered['Time (s)'], df_filtered['DOF3'], label=f'{activity} - DOF3')\n",
    "    plt.title(f'{activity} - Acelerómetro (3 DOF)')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Acceleration')\n",
    "    plt.legend()\n",
    "    plt.xlim(start_second, end_second)\n",
    "\n",
    "    # Subgráfico para el giroscopio (3 DOF)\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(df_filtered['Time (s)'], df_filtered['DOF4'], label=f'{activity} - DOF4')\n",
    "    plt.plot(df_filtered['Time (s)'], df_filtered['DOF5'], label=f'{activity} - DOF5')\n",
    "    plt.plot(df_filtered['Time (s)'], df_filtered['DOF6'], label=f'{activity} - DOF6')\n",
    "    plt.title(f'{activity} - Giroscopio (3 DOF)')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Angular Velocity')\n",
    "    plt.legend()\n",
    "    plt.xlim(start_second, end_second)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Script principal\n",
    "DATASET_NAME = 'hhar_20_120'  # Ajusta esto según tu dataset\n",
    "\n",
    "# Cargar datos y labels\n",
    "data, labels, activity_label_index, activity_label, user_label_index, user_label = load_data_and_labels(DATASET_NAME)\n",
    "\n",
    "print(f\"Forma de los datos: {data.shape}\")\n",
    "print(f\"Forma de las etiquetas: {labels.shape}\")\n",
    "print(f\"Etiquetas de actividad: {activity_label}\")\n",
    "\n",
    "# Crear DataFrame\n",
    "df = create_dataframe(data, labels, activity_label_index, activity_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualizar una muestra de los datos\n",
    "plot_data(df, start_second=0, end_second=60)  # Visualiza el primer minuto de datos\n",
    "\n",
    "# Visualizar una actividad específica\n",
    "#plot_activity(df, activity=activity_label[0], start_second=0, end_second=60)\n",
    "\n",
    "# Estadísticas de las labels\n",
    "print(\"\\nDistribución de labels:\")\n",
    "print(df['Activity'].value_counts(normalize=True))\n",
    "\n",
    "# Verificar la forma de los datos para el modelo\n",
    "print(f\"\\nForma de los datos para el modelo: {data.shape}\")\n",
    "print(f\"Cada ventana tiene {data.shape[1]} muestras de tiempo y {data.shape[2]} canales.\")\n",
    "print(f\"Duración de cada ventana: {data.shape[1]/20:.2f} segundos\")  # Asumiendo sr=20\n",
    "\n",
    "category, probabilities = classify_imu_data(raw_imu_data, seconds=6, sample_rate=20,bert_model=bert_model, classifier_model=classifier_model,dataset_config=dataset_config)\n",
    "print_detailed_results(category, probabilities, dataset_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_inference(df, start_second=None, end_second=None, sample_rate=20, window_size=120):\n",
    "    # Filtrar el DataFrame por tiempo si se especifica\n",
    "    if start_second is None:\n",
    "        start_second = df['Time (s)'].min()\n",
    "    if end_second is None:\n",
    "        end_second = df['Time (s)'].max()\n",
    "    \n",
    "    df_filtered = df[(df['Time (s)'] >= start_second) & (df['Time (s)'] <= end_second)]\n",
    "    \n",
    "    # Seleccionar solo las columnas de datos (DOF1 a DOF6)\n",
    "    data_columns = [f'DOF{i}' for i in range(1, 7)]\n",
    "    raw_imu_data = df_filtered[data_columns].values\n",
    "    \n",
    "    # Asegurarse de que tenemos suficientes datos para al menos una ventana\n",
    "    if len(raw_imu_data) < window_size:\n",
    "        raise ValueError(f\"No hay suficientes datos. Se necesitan al menos {window_size} muestras.\")\n",
    "    \n",
    "    # Recortar los datos para que sean múltiplos exactos del tamaño de la ventana\n",
    "    num_windows = len(raw_imu_data) // window_size\n",
    "    raw_imu_data = raw_imu_data[:num_windows * window_size]\n",
    "    \n",
    "    # Reshape los datos en ventanas\n",
    "    raw_imu_data = raw_imu_data.reshape(-1, window_size, 6)\n",
    "    \n",
    "    return raw_imu_data\n",
    "\n",
    "# Ejemplo de uso:\n",
    "def infer_from_dataframe(df, start_second, end_second, bert_model, classifier_model, dataset_config):\n",
    "    raw_imu_data = prepare_data_for_inference(df, start_second, end_second)\n",
    "    \n",
    "    categories = []\n",
    "    probabilities = []\n",
    "    \n",
    "    for window in raw_imu_data:\n",
    "        category, probability = classify_imu_data(\n",
    "            window, \n",
    "            seconds=6, \n",
    "            sample_rate=20, \n",
    "            bert_model=bert_model, \n",
    "            classifier_model=classifier_model,\n",
    "            dataset_config=dataset_config\n",
    "        )\n",
    "        categories.append(category)\n",
    "        probabilities.append(probability)\n",
    "    \n",
    "    return categories, probabilities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorías inferidas: ['bike', 'bike', 'bike', 'bike', 'bike', 'bike', 'bike', 'bike', 'bike', 'bike', 'bike', 'bike']\n",
      "Probabilidades: [array([0.52050996, 0.21361014, 0.00652829, 0.07260227, 0.0889217 ,\n",
      "       0.09782758], dtype=float32), array([0.51982397, 0.21413289, 0.00653117, 0.0727672 , 0.0887173 ,\n",
      "       0.09802746], dtype=float32), array([0.5206118 , 0.2135221 , 0.00652828, 0.0726058 , 0.08895447,\n",
      "       0.09777745], dtype=float32), array([0.520695  , 0.21345215, 0.00652845, 0.07257793, 0.08897798,\n",
      "       0.09776858], dtype=float32), array([0.5198233 , 0.21420579, 0.00652678, 0.07277516, 0.08873717,\n",
      "       0.09793176], dtype=float32), array([0.520083  , 0.21396807, 0.00652784, 0.07274228, 0.0888093 ,\n",
      "       0.0978694 ], dtype=float32), array([0.5206189 , 0.21352921, 0.00652753, 0.07259905, 0.08895869,\n",
      "       0.09776655], dtype=float32), array([0.5208229 , 0.21339713, 0.00652525, 0.07256636, 0.08903143,\n",
      "       0.09765697], dtype=float32), array([0.5209302 , 0.21330954, 0.00652504, 0.0725608 , 0.08906683,\n",
      "       0.09760758], dtype=float32), array([0.5209847 , 0.21327539, 0.00652419, 0.07256755, 0.08908758,\n",
      "       0.09756056], dtype=float32), array([0.5206655 , 0.21352272, 0.00652544, 0.07261933, 0.08898751,\n",
      "       0.09767957], dtype=float32), array([0.5207796 , 0.21340042, 0.00652714, 0.07258473, 0.0890116 ,\n",
      "       0.09769655], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# Uso\n",
    "start_second = 0\n",
    "end_second = 120  # Por ejemplo, inferir sobre el primer minuto de datos\n",
    "inicio = 2800\n",
    "tiempo = 120\n",
    "fin=inicio+ tiempo\n",
    "\n",
    "categories, probabilities = infer_from_dataframe(df, inicio, fin, bert_model, classifier_model, dataset_config)\n",
    "\n",
    "print(\"Categorías inferidas:\", categories)\n",
    "print(\"Probabilidades:\", probabilities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "limu-py38",
   "language": "python",
   "name": "limu-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
