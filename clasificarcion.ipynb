{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from models import LIMUBertModel4Pretrain, ClassifierGRU\n",
    "from config import PretrainModelConfig, ClassifierModelConfig, load_model_config, load_dataset_stats\n",
    "\n",
    "# Asumimos que estamos en un notebook de Colab/Jupyter\n",
    "\n",
    "# 1. Configuración y carga de modelos\n",
    "def load_models(bert_path, classifier_path, dataset, version):\n",
    "    # Cargar configuraciones\n",
    "    bert_cfg = load_model_config('pretrain', 'base', 'v1', path_bert='config/limu_bert.json')\n",
    "    classifier_cfg = load_model_config('classifier', 'gru', 'v1', path_classifier='config/classifier.json')\n",
    "    \n",
    "    # Imprimir la configuración del clasificador para verificar\n",
    "    print(\"Classifier config:\", classifier_cfg)\n",
    "    \n",
    "    # Inicializar modelos\n",
    "    bert_model = LIMUBertModel4Pretrain(bert_cfg, output_embed=True)\n",
    "    # Asumiendo que el modelo guardado tiene una capa lineal final de 6 a 10\n",
    "    output_dim = 6  # o el valor correcto basado en tu modelo guardado\n",
    "    classifier_model = ClassifierGRU(classifier_cfg, input=bert_cfg.hidden, output=output_dim)\n",
    "    #classifier_model = ClassifierGRU(classifier_cfg, input=bert_cfg.hidden, output=classifier_cfg.linear_io[-1][1])\n",
    "    \n",
    "    # Imprimir la estructura del modelo para verificar\n",
    "    print(classifier_model)\n",
    "    \n",
    "    # Cargar pesos pre-entrenados\n",
    "    bert_model.load_state_dict(torch.load(bert_path, map_location='cpu'))\n",
    "    \n",
    "    # Cargar el estado del clasificador\n",
    "    classifier_state = torch.load(classifier_path, map_location='cpu')\n",
    "    \n",
    "    # Imprimir las claves y formas del estado del clasificador\n",
    "    for key, value in classifier_state.items():\n",
    "        print(f\"{key}: {value.shape}\")\n",
    "    \n",
    "    # Intenta cargar el estado del clasificador\n",
    "    classifier_model.load_state_dict(classifier_state, strict=False)\n",
    "    \n",
    "    return bert_model, classifier_model\n",
    "\n",
    "# 2. Preprocesamiento de datos\n",
    "def preprocess_data(raw_data, window_size=120, stride=60):\n",
    "    windows = []\n",
    "    for i in range(0, len(raw_data) - window_size + 1, stride):\n",
    "        windows.append(raw_data[i:i+window_size])\n",
    "    return np.array(windows)\n",
    "\n",
    "# 3. Inferencia\n",
    "def inference(bert_model, classifier_model, input_data):\n",
    "    bert_model.eval()\n",
    "    classifier_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embeddings = bert_model(torch.FloatTensor(input_data))\n",
    "        outputs = classifier_model(embeddings)\n",
    "        \n",
    "    return torch.softmax(outputs, dim=1).numpy()\n",
    "\n",
    "# 4. Función principal modificada\n",
    "def classify_imu_data(raw_data, seconds, sample_rate, bert_path, classifier_path, dataset, version):\n",
    "    # Cargar modelos\n",
    "    bert_model, classifier_model = load_models(bert_path, classifier_path, dataset, version)\n",
    "    \n",
    "    # Cargar configuración del dataset\n",
    "    dataset_config = load_dataset_stats(dataset, version)\n",
    "    \n",
    "    # Preprocesar datos\n",
    "    n_samples = seconds * sample_rate\n",
    "    data = raw_data[:n_samples]  # Tomar solo los primeros N segundos\n",
    "    preprocessed_data = preprocess_data(data, window_size=dataset_config.seq_len, stride=dataset_config.seq_len//2)\n",
    "    \n",
    "    # Realizar inferencia\n",
    "    predictions = inference(bert_model, classifier_model, preprocessed_data)\n",
    "    \n",
    "    # Agregar predicciones\n",
    "    final_prediction = np.mean(predictions, axis=0)\n",
    "    \n",
    "    # Obtener la categoría con la probabilidad más alta\n",
    "    category_index = np.argmax(final_prediction)\n",
    "    category = dataset_config.activity_label[category_index]\n",
    "    \n",
    "    return category, final_prediction\n",
    "\n",
    "# 5. Función para imprimir resultados detallados\n",
    "def print_detailed_results(category, probabilities, dataset_config):\n",
    "    print(f\"Categoría predicha: {category}\")\n",
    "    print(\"\\nProbabilidades por categoría:\")\n",
    "    for label, prob in zip(dataset_config.activity_label, probabilities):\n",
    "        print(f\"{label}: {prob:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de uso modificado\n",
    "BERT_PATH = 'saved/pretrain_base_motion_20_120/motion.pt'\n",
    "CLASSIFIER_PATH = 'saved/classifier_base_gru_motion_20_120/motion.pt'\n",
    "DATASET = 'hhar'\n",
    "VERSION = '20_120'\n",
    "\n",
    "# Simular datos de IMU (reemplaza esto con tus datos reales)\n",
    "raw_imu_data = np.random.randn(120, 6)  # 6 seg de datos a 20Hz\n",
    "\n",
    "# Clasificar 10 segundos de datos\n",
    "category, probabilities = classify_imu_data(raw_imu_data, seconds=10, sample_rate=100, \n",
    "                                            bert_path=BERT_PATH, classifier_path=CLASSIFIER_PATH,\n",
    "                                            dataset=DATASET, version=VERSION)\n",
    "\n",
    "# Cargar la configuración del dataset para obtener las etiquetas\n",
    "dataset_config = load_dataset_stats(DATASET, VERSION)\n",
    "\n",
    "# Imprimir resultados detallados\n",
    "print_detailed_results(category, probabilities, dataset_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## desacoplando\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from models import LIMUBertModel4Pretrain, ClassifierGRU\n",
    "from config import PretrainModelConfig, ClassifierModelConfig, load_model_config, load_dataset_stats\n",
    "\n",
    "\n",
    "# 1. Configuración y carga de modelos\n",
    "def load_models(bert_path, classifier_path, dataset, version):\n",
    "    # Cargar configuraciones\n",
    "    bert_cfg = load_model_config('pretrain', 'base', 'v1', path_bert='config/limu_bert.json')\n",
    "    classifier_cfg = load_model_config('classifier', 'gru', 'v1', path_classifier='config/classifier.json')\n",
    "    \n",
    "    # Imprimir la configuración del clasificador para verificar\n",
    "    print(\"Classifier config:\", classifier_cfg)\n",
    "    \n",
    "    # Inicializar modelos\n",
    "    bert_model = LIMUBertModel4Pretrain(bert_cfg, output_embed=True)\n",
    "    # Asumiendo que el modelo guardado tiene una capa lineal final de 6 a 10\n",
    "    output_dim = 6  # o el valor correcto basado en tu modelo guardado\n",
    "    classifier_model = ClassifierGRU(classifier_cfg, input=bert_cfg.hidden, output=output_dim)\n",
    "    #classifier_model = ClassifierGRU(classifier_cfg, input=bert_cfg.hidden, output=classifier_cfg.linear_io[-1][1])\n",
    "    \n",
    "    # Imprimir la estructura del modelo para verificar\n",
    "    print(classifier_model)\n",
    "    \n",
    "    # Cargar pesos pre-entrenados\n",
    "    bert_model.load_state_dict(torch.load(bert_path, map_location='cpu'))\n",
    "    \n",
    "    # Cargar el estado del clasificador\n",
    "    classifier_state = torch.load(classifier_path, map_location='cpu')\n",
    "    \n",
    "    # Imprimir las claves y formas del estado del clasificador\n",
    "    for key, value in classifier_state.items():\n",
    "        print(f\"{key}: {value.shape}\")\n",
    "    \n",
    "    # Intenta cargar el estado del clasificador\n",
    "    classifier_model.load_state_dict(classifier_state, strict=False)\n",
    "    \n",
    "    return bert_model, classifier_model\n",
    "\n",
    "# 2. Preprocesamiento de datos\n",
    "def preprocess_data(raw_data, window_size=120, stride=60):\n",
    "    windows = []\n",
    "    for i in range(0, len(raw_data) - window_size + 1, stride):\n",
    "        windows.append(raw_data[i:i+window_size])\n",
    "    return np.array(windows)\n",
    "\n",
    "# 3. Inferencia (modificada para aceptar modelos precargados)\n",
    "def inference(bert_model, classifier_model, input_data):\n",
    "    bert_model.eval()\n",
    "    classifier_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embeddings = bert_model(torch.FloatTensor(input_data))\n",
    "        outputs = classifier_model(embeddings)\n",
    "        \n",
    "    return torch.softmax(outputs, dim=1).numpy()\n",
    "\n",
    "# 4. Función de clasificación (modificada para aceptar modelos precargados)\n",
    "def classify_imu_data(raw_data, seconds, sample_rate, bert_model, classifier_model, dataset_config):\n",
    "    # Preprocesar datos\n",
    "    n_samples = seconds * sample_rate\n",
    "    data = raw_data[:n_samples]  # Tomar solo los primeros N segundos\n",
    "    preprocessed_data = preprocess_data(data, window_size=dataset_config.seq_len, stride=dataset_config.seq_len//2)\n",
    "    \n",
    "    # Realizar inferencia\n",
    "    predictions = inference(bert_model, classifier_model, preprocessed_data)\n",
    "    \n",
    "    # Agregar predicciones\n",
    "    final_prediction = np.mean(predictions, axis=0)\n",
    "    \n",
    "    # Obtener la categoría con la probabilidad más alta\n",
    "    category_index = np.argmax(final_prediction)\n",
    "    category = dataset_config.activity_label[category_index]\n",
    "    \n",
    "    return category, final_prediction\n",
    "\n",
    "\n",
    "# 5. Función para imprimir resultados detallados\n",
    "def print_detailed_results(category, probabilities, dataset_config):\n",
    "    print(f\"Categoría predicha: {category}\")\n",
    "    print(\"\\nProbabilidades por categoría:\")\n",
    "    for label, prob in zip(dataset_config.activity_label, probabilities):\n",
    "        print(f\"{label}: {prob:.4f}\")\n",
    "\n",
    "\n",
    "# 6. Nueva función para cargar modelos y configuración\n",
    "def load_models_and_config(bert_path, classifier_path, dataset, version):\n",
    "    bert_model, classifier_model = load_models(bert_path, classifier_path, dataset, version)\n",
    "    dataset_config = load_dataset_stats(dataset, version)\n",
    "    return bert_model, classifier_model, dataset_config\n",
    "\n",
    "# Función para realizar múltiples inferencias\n",
    "def run_multiple_inferences(num_inferences):\n",
    "    for i in range(num_inferences):\n",
    "        # Simular datos de IMU (reemplaza esto con tus datos reales)\n",
    "        raw_imu_data = np.random.randn(6000, 6)  # 1 minuto de datos a 100Hz\n",
    "\n",
    "        # Clasificar 10 segundos de datos\n",
    "        category, probabilities = classify_imu_data(raw_imu_data, seconds=10, sample_rate=100, \n",
    "                                                    bert_model=bert_model, classifier_model=classifier_model,\n",
    "                                                    dataset_config=dataset_config)\n",
    "\n",
    "        print(f\"\\nInferencia {i+1}:\")\n",
    "        print_detailed_results(category, probabilities, dataset_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ejemplo de uso modificado\n",
    "BERT_PATH = 'saved/pretrain_base_motion_20_120/motion.pt'\n",
    "CLASSIFIER_PATH = 'saved/classifier_base_gru_motion_20_120/motion.pt'\n",
    "DATASET = 'hhar'\n",
    "VERSION = '20_120'\n",
    "\n",
    "# Cargar modelos y configuración una sola vez\n",
    "bert_model, classifier_model, dataset_config = load_models_and_config(BERT_PATH, CLASSIFIER_PATH, DATASET, VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_imu_data = np.random.randn(120, 6)  # 6 seg de datos a 20Hz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categoría predicha: bike\n",
      "\n",
      "Probabilidades por categoría:\n",
      "bike: 0.5448\n",
      "sit: 0.1550\n",
      "downstairs: 0.0097\n",
      "upstairs: 0.0650\n",
      "stand: 0.0950\n",
      "walk: 0.1306\n"
     ]
    }
   ],
   "source": [
    "category, probabilities = classify_imu_data(raw_imu_data, seconds=6, sample_rate=20,bert_model=bert_model, classifier_model=classifier_model,dataset_config=dataset_config)\n",
    "print_detailed_results(category, probabilities, dataset_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inferencia 1:\n",
      "Categoría predicha: bike\n",
      "\n",
      "Probabilidades por categoría:\n",
      "bike: 0.4348\n",
      "sit: 0.1414\n",
      "downstairs: 0.0201\n",
      "upstairs: 0.0569\n",
      "stand: 0.0901\n",
      "walk: 0.2567\n",
      "\n",
      "Inferencia 2:\n",
      "Categoría predicha: bike\n",
      "\n",
      "Probabilidades por categoría:\n",
      "bike: 0.4334\n",
      "sit: 0.1562\n",
      "downstairs: 0.0246\n",
      "upstairs: 0.0897\n",
      "stand: 0.0987\n",
      "walk: 0.1973\n",
      "\n",
      "Inferencia 3:\n",
      "Categoría predicha: bike\n",
      "\n",
      "Probabilidades por categoría:\n",
      "bike: 0.5128\n",
      "sit: 0.1500\n",
      "downstairs: 0.0133\n",
      "upstairs: 0.0634\n",
      "stand: 0.1001\n",
      "walk: 0.1604\n"
     ]
    }
   ],
   "source": [
    "run_multiple_inferences(3)  # Cambia el número según cuántas inferencias quieras realizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'dataset/hhar_20_120'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 125\u001b[0m\n\u001b[1;32m    122\u001b[0m DATASET_NAME \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdataset/hhar_20_120\u001b[39m\u001b[39m'\u001b[39m  \u001b[39m# Ajusta esto según tu dataset\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m# Cargar datos y labels\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m data, labels, activity_label_index, activity_label, user_label_index, user_label \u001b[39m=\u001b[39m load_data_and_labels(DATASET_NAME)\n\u001b[1;32m    127\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mForma de los datos: \u001b[39m\u001b[39m{\u001b[39;00mdata\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    128\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mForma de las etiquetas: \u001b[39m\u001b[39m{\u001b[39;00mlabels\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[22], line 18\u001b[0m, in \u001b[0;36mload_data_and_labels\u001b[0;34m(dataset_name)\u001b[0m\n\u001b[1;32m     15\u001b[0m config_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(cwd, \u001b[39m'\u001b[39m\u001b[39mdataset/data_config.json\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m config \u001b[39m=\u001b[39m load_config(config_path)\n\u001b[0;32m---> 18\u001b[0m data_info \u001b[39m=\u001b[39m config[dataset_name]\n\u001b[1;32m     19\u001b[0m sr \u001b[39m=\u001b[39m data_info[\u001b[39m'\u001b[39m\u001b[39msr\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     20\u001b[0m seq_len \u001b[39m=\u001b[39m data_info[\u001b[39m'\u001b[39m\u001b[39mseq_len\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'dataset/hhar_20_120'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Funciones proporcionadas\n",
    "def load_config(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        config = json.load(f)\n",
    "    return config\n",
    "\n",
    "def load_data_and_labels(dataset_name):\n",
    "    cwd = os.getcwd()\n",
    "    config_path = os.path.join(cwd, 'dataset/data_config.json')\n",
    "    config = load_config(config_path)\n",
    "\n",
    "    data_info = config[dataset_name]\n",
    "    sr = data_info['sr']\n",
    "    seq_len = data_info['seq_len']\n",
    "    dimension = data_info['dimension']\n",
    "    activity_label_index = data_info['activity_label_index']\n",
    "    activity_label = data_info['activity_label']\n",
    "    user_label_index = data_info.get('user_label_index')\n",
    "    user_label = data_info.get('user_label')\n",
    "    \n",
    "    data_file = f'data_{sr}_{seq_len}.npy'\n",
    "    label_file = f'label_{sr}_{seq_len}.npy'\n",
    "    \n",
    "    dataset_folder = os.path.join(cwd,\"dataset/\",dataset_name.split('_')[0])\n",
    "    data_path = os.path.join(cwd, dataset_folder, data_file)\n",
    "    label_path = os.path.join(cwd, dataset_folder, label_file)\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"Data file not found: {data_path}\")\n",
    "    if not os.path.exists(label_path):\n",
    "        raise FileNotFoundError(f\"Label file not found: {label_path}\")\n",
    "\n",
    "    data = np.load(data_path)\n",
    "    labels = np.load(label_path)\n",
    "    \n",
    "    return data, labels, activity_label_index, activity_label, user_label_index, user_label\n",
    "\n",
    "def create_dataframe(data, labels, activity_label_index, activity_label):\n",
    "    n_samples, n_windows, n_features = data.shape\n",
    "    time_index = np.arange(n_samples * n_windows) / (n_windows // 10)  # Calcula los tiempos en segundos\n",
    "    \n",
    "    # Aplanar los datos y las etiquetas\n",
    "    flattened_data = data.reshape(n_samples * n_windows, n_features)\n",
    "    flattened_labels = labels.reshape(n_samples * n_windows, -1)\n",
    "    \n",
    "    # Crear el DataFrame con los datos y las etiquetas\n",
    "    df = pd.DataFrame(flattened_data, columns=[f'DOF{i+1}' for i in range(n_features)])\n",
    "    df['Time (s)'] = time_index\n",
    "    df['Activity'] = [activity_label[int(label)] for label in flattened_labels[:, activity_label_index]]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def plot_data(df, start_second=None, end_second=None, figsize=(12, 8), show_milliseconds=False):\n",
    "    if start_second is None:\n",
    "        start_second = df['Time (s)'].min()\n",
    "    if end_second is None:\n",
    "        end_second = df['Time (s)'].max()\n",
    "\n",
    "    df_filtered = df[(df['Time (s)'] >= start_second) & (df['Time (s)'] <= end_second)]\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    for i in range(1, df.shape[1] - 1):  # Excluye la columna 'Activity'\n",
    "        plt.subplot(3, 2, i)\n",
    "        for activity in df['Activity'].unique():\n",
    "            activity_data = df_filtered[df_filtered['Activity'] == activity]\n",
    "            plt.plot(activity_data['Time (s)'], activity_data[f'DOF{i}'], label=activity)\n",
    "        plt.title(f'DOF{i}')\n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.ylabel(f'DOF{i}')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.xticks(ticks=plt.xticks()[0], labels=[f'{x:.1f}s' for x in plt.xticks()[0]])\n",
    "        if show_milliseconds:\n",
    "            plt.xticks(ticks=plt.xticks()[0], labels=[f'{x:.1f}s ({x*1000:.0f}ms)' for x in plt.xticks()[0]])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_activity(df, activity, start_second=None, end_second=None, figsize=(12, 8)):\n",
    "    if start_second is None:\n",
    "        start_second = df['Time (s)'].min()\n",
    "    if end_second is None:\n",
    "        end_second = df['Time (s)'].max()\n",
    "\n",
    "    df_filtered = df[(df['Time (s)'] >= start_second) & (df['Time (s)'] <= end_second) & (df['Activity'] == activity)]\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    # Subgráfico para el acelerómetro (3 DOF)\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(df_filtered['Time (s)'], df_filtered['DOF1'], label=f'{activity} - DOF1')\n",
    "    plt.plot(df_filtered['Time (s)'], df_filtered['DOF2'], label=f'{activity} - DOF2')\n",
    "    plt.plot(df_filtered['Time (s)'], df_filtered['DOF3'], label=f'{activity} - DOF3')\n",
    "    plt.title(f'{activity} - Acelerómetro (3 DOF)')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Acceleration')\n",
    "    plt.legend()\n",
    "    plt.xlim(start_second, end_second)\n",
    "\n",
    "    # Subgráfico para el giroscopio (3 DOF)\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(df_filtered['Time (s)'], df_filtered['DOF4'], label=f'{activity} - DOF4')\n",
    "    plt.plot(df_filtered['Time (s)'], df_filtered['DOF5'], label=f'{activity} - DOF5')\n",
    "    plt.plot(df_filtered['Time (s)'], df_filtered['DOF6'], label=f'{activity} - DOF6')\n",
    "    plt.title(f'{activity} - Giroscopio (3 DOF)')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Angular Velocity')\n",
    "    plt.legend()\n",
    "    plt.xlim(start_second, end_second)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Script principal\n",
    "DATASET_NAME = 'hhar_20_120'  # Ajusta esto según tu dataset\n",
    "\n",
    "# Cargar datos y labels\n",
    "data, labels, activity_label_index, activity_label, user_label_index, user_label = load_data_and_labels(DATASET_NAME)\n",
    "\n",
    "print(f\"Forma de los datos: {data.shape}\")\n",
    "print(f\"Forma de las etiquetas: {labels.shape}\")\n",
    "print(f\"Etiquetas de actividad: {activity_label}\")\n",
    "\n",
    "# Crear DataFrame\n",
    "df = create_dataframe(data, labels, activity_label_index, activity_label)\n",
    "\n",
    "# Visualizar una muestra de los datos\n",
    "plot_data(df, start_second=0, end_second=60)  # Visualiza el primer minuto de datos\n",
    "\n",
    "# Visualizar una actividad específica\n",
    "plot_activity(df, activity=activity_label[0], start_second=0, end_second=60)\n",
    "\n",
    "# Estadísticas de las labels\n",
    "print(\"\\nDistribución de labels:\")\n",
    "print(df['Activity'].value_counts(normalize=True))\n",
    "\n",
    "# Verificar la forma de los datos para el modelo\n",
    "print(f\"\\nForma de los datos para el modelo: {data.shape}\")\n",
    "print(f\"Cada ventana tiene {data.shape[1]} muestras de tiempo y {data.shape[2]} canales.\")\n",
    "print(f\"Duración de cada ventana: {data.shape[1]/20:.2f} segundos\")  # Asumiendo sr=20"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "limu-py38",
   "language": "python",
   "name": "limu-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
